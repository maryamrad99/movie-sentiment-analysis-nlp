{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82f9194d",
   "metadata": {},
   "source": [
    "# Final Pipeline for Text Data\n",
    "\n",
    "## Objective:\n",
    "Goal is to:\n",
    "1. Combine all preprocessing steps into one pipeline\n",
    "2. Apply it to the IMDb dataset\n",
    "3. Save cleaned dataset for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5653ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import libraries\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK resources (first run only)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dadbd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define preprocessing pipeline\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text, use_stemming=False):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # Tokenization\n",
    "    # tokens = word_tokenize(text)\n",
    "    tokens = text.split()  # Simple fallback tokenizer\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Stemming or Lemmatization\n",
    "    if use_stemming:\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "    else:\n",
    "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    \n",
    "    return \" \".join(tokens)  # join back into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134b0483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load dataset (subset for demo, can increase later)\n",
    "\n",
    "base_dir = \"/Users/mimi/aclImdb\"\n",
    "train_pos_dir = os.path.join(base_dir, \"train/pos\")\n",
    "train_neg_dir = os.path.join(base_dir, \"train/neg\")\n",
    "\n",
    "def load_reviews(directory, label, limit=1000):\n",
    "    data = []\n",
    "    for i, fname in enumerate(os.listdir(directory)):\n",
    "        if i >= limit:\n",
    "            break\n",
    "        with open(os.path.join(directory, fname), encoding=\"utf-8\") as f:\n",
    "            data.append((f.read(), label))\n",
    "    return data\n",
    "\n",
    "pos_reviews = load_reviews(train_pos_dir, 1, limit=1000)\n",
    "neg_reviews = load_reviews(train_neg_dir, 0, limit=1000)\n",
    "\n",
    "all_data = pos_reviews + neg_reviews\n",
    "df = pd.DataFrame(all_data, columns=[\"review\", \"label\"])\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9c9c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Apply preprocessing\n",
    "\n",
    "df[\"cleaned_review\"] = df[\"review\"].apply(lambda x: preprocess_text(x, use_stemming=False))\n",
    "\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9b3c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Save cleaned dataset\n",
    "\n",
    "df.to_csv(\"cleaned_imdb_reviews.csv\", index=False)\n",
    "print(\" Cleaned dataset saved as cleaned_imdb_reviews.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f673bcf",
   "metadata": {},
   "source": [
    "# Dfter running this, you should have a CSV file with:\n",
    "- ``review`` (original text)\n",
    "- ``label`` (0=negative, 1=positive)\n",
    "- ``cleaned_review`` (fully preprocessed text)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
