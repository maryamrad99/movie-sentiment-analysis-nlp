{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54999438",
   "metadata": {},
   "source": [
    "# Model Evaluation and Comparison\n",
    "\n",
    "## Objective\n",
    "To evaluate SVM performance, compare it with Naive Bayes, and prepare for broader model comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77cc83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Step 2: Collect metrics for Naive Bayes & SVM\n",
    "# (replace these with your actual results from Oct 2 and Oct 5)\n",
    "nb_results = {\n",
    "    \"Model\": \"Naive Bayes\",\n",
    "    \"Accuracy\": 0.825,\n",
    "    \"Precision\": 0.83,\n",
    "    \"Recall\": 0.82,\n",
    "    \"F1\": 0.82\n",
    "}\n",
    "\n",
    "svm_results = {\n",
    "    \"Model\": \"SVM\",\n",
    "    \"Accuracy\": 0.815,\n",
    "    \"Precision\": 0.82,\n",
    "    \"Recall\": 0.81,\n",
    "    \"F1\": 0.81\n",
    "}\n",
    "\n",
    "# Step 3: Create comparison table\n",
    "results_df = pd.DataFrame([nb_results, svm_results])\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55de7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Visualize results\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"]\n",
    "\n",
    "for metric in metrics:\n",
    "    plt.bar(results_df[\"Model\"], results_df[metric])\n",
    "    plt.title(f\"{metric} Comparison\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.ylim(0.7, 1.0)   # zoomed to highlight differences\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
